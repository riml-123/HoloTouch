import cv2
import mediapipe as mp
import numpy as np
import onnxruntime as ort
import time
from collections import deque
import math

# ================= âš™ï¸ ì„¤ì • (Configuration) =================
# ëª¨ë¸ ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
MODEL_PATH = r"code\HoloTouch_SE_TCN.onnx" 

INPUT_CHANNELS = 126
MAX_SEQ_LEN = 40
NUM_CLASSES = 8

TARGET_LABELS = [
    "No gesture", "Doing other things",
    "Zooming In With Two Fingers", "Zooming Out With Two Fingers",
    "Zooming In With Full Hand", "Zooming Out With Full Hand",
    "Thumb Up", "Thumb Down" 
]

CONFIDENCE_THRESHOLD = 0.8
ACTION_COOLDOWN = 0.5
Z_TOUCH_THRESHOLD = -0.05
MOVE_SCALE = 1.5 # ì´ë™ ìŠ¤ì¼€ì¼ ì¦ê°€ (ë¯¼ê°ë„ ìƒìŠ¹)
# SMOOTHING_FACTOR ëŒ€ì‹  OneEuro Filter íŒŒë¼ë¯¸í„° ì‚¬ìš©

# ================= ğŸ“ˆ OneEuro Filter (ì‹ í˜¸ ì²˜ë¦¬ ê³ ë„í™”) =================
class OneEuroFilter:
    """ 
    ì†ë„ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ í•„í„°ë§ì„ ì¡°ì ˆí•˜ì—¬ ë–¨ë¦¼ ë°©ì§€(Jitter)ì™€ 
    ë°˜ì‘ ì§€ì—°(Lag)ì„ ìµœì†Œí™”í•˜ëŠ” ê³ ê¸‰ í•„í„° 
    """
    def __init__(self, t0, x0, min_cutoff=0.01, beta=0.5, d_cutoff=1.0):
        self.min_cutoff = min_cutoff
        self.beta = beta
        self.d_cutoff = d_cutoff
        self.x_prev = x0
        self.dx_prev = 0.0
        self.t_prev = t0

    def smoothing_factor(self, t_e, cutoff):
        r = 2 * math.pi * cutoff * t_e
        return r / (r + 1)

    def exponential_smoothing(self, a, x, x_prev):
        return a * x + (1 - a) * x_prev

    def __call__(self, t, x):
        t_e = t - self.t_prev
        if t_e <= 0: 
            return self.x_prev

        # 1. ì†ë„ ì¶”ì • (1ì°¨ ë¯¸ë¶„)
        dx = (x - self.x_prev) / t_e
        dx_hat = self.exponential_smoothing(self.smoothing_factor(t_e, self.d_cutoff), dx, self.dx_prev)

        # 2. cutoff ì£¼íŒŒìˆ˜ ë™ì  ì¡°ì ˆ
        # ì†ë„ê°€ ë¹ ë¥´ë©´ cutoffë¥¼ ë†’ì—¬ í•„í„°ë§ì„ ì•½í•˜ê²Œ í•¨ (ë°˜ì‘ì„± ì¦ê°€)
        cutoff = self.min_cutoff + self.beta * abs(dx_hat)
        
        # 3. ìµœì¢… í•„í„°ë§
        a = self.smoothing_factor(t_e, cutoff)
        x_hat = self.exponential_smoothing(a, x, self.x_prev)

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.x_prev = x_hat
        self.dx_prev = dx_hat
        self.t_prev = t
        return x_hat

# ================= ğŸ§® ìœ í‹¸ë¦¬í‹° & ì „ì²˜ë¦¬ =================

def normalize_skeleton(data):
    """ TCN ëª¨ë¸ ì…ë ¥ì— ë§ëŠ” ì†ëª© ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ ë° ìŠ¤ì¼€ì¼ ì •ê·œí™” """
    T = data.shape[0]
    skeleton = data.reshape(T, 21, 3)
    wrist = skeleton[:, 0:1, :]
    skeleton = skeleton - wrist
    # ì†ëª©-ì¤‘ì§€ ì†ê°€ë½ ë (9ë²ˆ) ê±°ë¦¬ë¡œ ìŠ¤ì¼€ì¼ ì •ê·œí™”
    dist = np.linalg.norm(skeleton[:, 9, :] - skeleton[:, 0, :], axis=1, keepdims=True) + 1e-6
    skeleton = skeleton / dist[:, :, np.newaxis]
    return skeleton.reshape(T, -1)

def preprocess_buffer(buffer):
    """ ë²„í¼ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ TCN ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ """
    features = np.array(buffer, dtype=np.float32)
    features = normalize_skeleton(features)
    
    # ì†ë„(Velocity) íŠ¹ì§• ì¶”ê°€
    velocity = np.zeros_like(features)
    if features.shape[0] > 1:
        velocity[1:] = features[1:] - features[:-1]
        
    combined = np.concatenate([features, velocity], axis=1) # 63 * 2 = 126 ì±„ë„
    
    # íŒ¨ë”©/íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸°
    seq_len = combined.shape[0]
    if seq_len < MAX_SEQ_LEN:
        pad_len = MAX_SEQ_LEN - seq_len
        padding = np.zeros((pad_len, INPUT_CHANNELS), dtype=np.float32)
        combined = np.vstack([combined, padding])
    else:
        combined = combined[-MAX_SEQ_LEN:, :]
        
    # TCN ì…ë ¥ í˜•ì‹ (N, C, T)ë¡œ ë³€í™˜
    combined = combined.transpose(1, 0) 
    input_data = np.expand_dims(combined, axis=0).astype(np.float32)
    return input_data

def softmax(x):
    """ Softmax í•¨ìˆ˜ êµ¬í˜„ """
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=1, keepdims=True)

def draw_dashboard(image, label, conf, fps):
    """ í™”ë©´ ìƒë‹¨ì— AI ì¶”ë¡  ê²°ê³¼ ë° FPS ëŒ€ì‹œë³´ë“œ ê·¸ë¦¬ê¸° """
    overlay = image.copy()
    cv2.rectangle(overlay, (0, 0), (380, 90), (0, 0, 0), -1) 
    alpha = 0.6
    cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)

    is_confident = conf > CONFIDENCE_THRESHOLD
    color = (0, 255, 0) if is_confident else (180, 180, 180)
    display_label = label if is_confident else "Waiting..."
    
    cv2.putText(image, f"AI: {display_label}", (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
    
    bar_width = 200
    filled_width = int(bar_width * conf)
    cv2.rectangle(image, (15, 50), (15 + bar_width, 65), (50, 50, 50), -1)
    cv2.rectangle(image, (15, 50), (15 + filled_width, 65), color, -1)
    cv2.rectangle(image, (15, 50), (15 + bar_width, 65), (200, 200, 200), 1)
    
    cv2.putText(image, f"{conf*100:.1f}%", (230, 62), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    cv2.putText(image, f"FPS: {int(fps)}", (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)

# ================= ğŸ® ì¸í„°ë™ì…˜ ê´€ë¦¬ì (InteractionManager) =================
class InteractionManager:
    def __init__(self, screen_w, screen_h):
        self.w, self.h = screen_w, screen_h
        
        # UI ìƒíƒœ
        self.ui_x, self.ui_y = screen_w // 2, screen_h // 2
        self.ui_size = 150
        self.ui_color = (255, 0, 0)
        self.is_selected = False
        
        # Grab ìƒíƒœ
        self.is_grabbing = False
        self.grab_anchor_wrist = None 
        self.grab_anchor_ui = None
        self.release_counter = 0      
        
        # ìŠ¤ì™€ì´í•‘ ìƒíƒœ
        self.last_action_time = 0
        self.prev_hand_pos = None # ì† ì¤‘ì‹¬ (9ë²ˆ) ì¢Œí‘œ

        # ì¢Œí‘œ í•„í„°ë§ (OneEuro Filter)
        now = time.time()
        # í¬ì¸í„°(ê²€ì§€) í•„í„°: ë¯¼ê°ë„ ë†’ìŒ (beta=1.0)
        self.filter_idx_x = OneEuroFilter(now, self.ui_x, min_cutoff=0.01, beta=1.0)
        self.filter_idx_y = OneEuroFilter(now, self.ui_y, min_cutoff=0.01, beta=1.0)
        # ì†ëª© í•„í„°: ì•ˆì •ì„± ì¤‘ìš” (beta=0.5)
        self.filter_wrist_x = OneEuroFilter(now, self.ui_x, min_cutoff=0.05, beta=0.5)
        self.filter_wrist_y = OneEuroFilter(now, self.ui_y, min_cutoff=0.05, beta=0.5)

    def _get_pinch_center(self, landmarks):
        """ ì•µì»¤ í¬ì¸íŠ¸ (ì—„ì§€-ê²€ì§€ ì¤‘ì ) """
        thumb = landmarks.landmark[4]
        index = landmarks.landmark[8]
        avg_x = (thumb.x + index.x) / 2 * self.w
        avg_y = (thumb.y + index.y) / 2 * self.h
        return (avg_x, avg_y)

    def is_point_in_ui(self, x, y):
        """ ì¢Œí‘œê°€ UI ë°•ìŠ¤ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸ """
        half = self.ui_size // 2
        return (self.ui_x - half < x < self.ui_x + half) and \
               (self.ui_y - half < y < self.ui_y + half)

    def process(self, ai_label, ai_conf, landmarks, image):
        current_time = time.time()
        
        # 1. ëœë“œë§ˆí¬ ì¶”ì¶œ ë° OneEuro Filter ì ìš©
        idx_tip = landmarks.landmark[8]
        wrist = landmarks.landmark[0] 
        hand_center = landmarks.landmark[9]
        
        # raw ì¢Œí‘œ
        raw_ix, raw_iy = int(idx_tip.x * self.w), int(idx_tip.y * self.h)
        raw_wx, raw_wy = int(wrist.x * self.w), int(wrist.y * self.h)
        cx, cy = int(hand_center.x * self.w), int(hand_center.y * self.h)
        
        # OneEuro Filter ì ìš© (í¬ì¸í„°)
        ix = int(self.filter_idx_x(current_time, raw_ix))
        iy = int(self.filter_idx_y(current_time, raw_iy))
        
        # OneEuro Filter ì ìš© (ì†ëª© - Grab ì´ë™ìš©)
        wx = int(self.filter_wrist_x(current_time, raw_wx))
        wy = int(self.filter_wrist_y(current_time, raw_wy))
        
        # íŒì •ìš© ì•µì»¤ (ì—„ì§€-ê²€ì§€ ì¤‘ì )
        grab_x, grab_y = self._get_pinch_center(landmarks)
        
        # í¬ì¸í„° ê·¸ë¦¬ê¸° (ê²€ì§€ ë - íŒŒë€ ì )
        cv2.circle(image, (ix, iy), 8, (255, 0, 0), -1) 

        # =========================================================
        # [ìš°ì„ ìˆœìœ„ 1] Grab & Move (ì†ëª© ê¸°ì¤€ ì•ˆì •í™”)
        # =========================================================
        
        # 1-1. Grab ì‹œì‘: 'Full Hand Zoom In' ì œìŠ¤ì²˜, UI ìœ„ì—ì„œ
        confident = ai_conf > CONFIDENCE_THRESHOLD

        # 1-1. Grab ì‹œì‘: 'Full Hand Zoom In' + ê²€ì§€ê°€ UI ìœ„
        if ai_label == "Zooming In With Full Hand" and confident:
            # === í•µì‹¬ ìˆ˜ì •: íŒì • í¬ì¸íŠ¸ë¥¼ ix, iy (ê²€ì§€ ë)ë¡œ ë³€ê²½ ===
            if not self.is_grabbing and self.is_point_in_ui(ix, iy): 
                self.is_grabbing = True
                self.grab_anchor_wrist = (wx, wy) 
                self.grab_anchor_ui = (self.ui_x, self.ui_y)
                self.release_counter = 0 
                # Grab í™œì„±í™” ì‹œ UIëŠ” ë¬´ì¡°ê±´ ë…¸ë€ìƒ‰ (Grab ì¤‘)
                self.ui_color = (0, 255, 255) 
                print("âœŠ Grab Started! (Anchored at Filtered Wrist)")
                
            # Grab ìƒíƒœë¥¼ ìœ ì§€í•  ë•Œ, Release ì¹´ìš´í„°ë¥¼ ë¦¬ì…‹ (Grab ì œìŠ¤ì²˜ ìœ ì§€ ì¤‘)
            elif self.is_grabbing:
                 self.release_counter = 0

        # 1-2. Release ì²´í¬: 'Full Hand Zoom Out' (ë””ë°”ìš´ìŠ¤ ì ìš©)
        elif ai_label == "Zooming Out With Full Hand" and confident:
            if self.is_grabbing:
                self.release_counter += 1
                if self.release_counter > 3: # ì—°ì† 3í”„ë ˆì„ ì´ìƒ 'ë†“ê¸°' ê°ì§€
                    self.is_grabbing = False
                    self.grab_anchor_wrist = None
                    # Release í›„ UI ìƒ‰ìƒì€ ì„ íƒ ìƒíƒœì— ë”°ë¼ ë³µì›
                    self.ui_color = (0, 255, 0) if self.is_selected else (255, 0, 0)
                    self.release_counter = 0
                    print("ğŸ– Released (Confirmed)")
        
        else:
            # ë‹¤ë¥¸ ì œìŠ¤ì²˜ì´ê±°ë‚˜ ì œìŠ¤ì²˜ê°€ ì—†ì„ ë•Œ:
            # Grab ì¤‘ì¼ ê²½ìš°, Release ì œìŠ¤ì²˜ê°€ ì•„ë‹ˆë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹í•˜ê³  Grab ìœ ì§€
            if self.is_grabbing:
                self.release_counter = 0
                
        # 1-3. ì´ë™ ë¡œì§ (Grab ìƒíƒœ)
        if self.is_grabbing:
            # UI ìƒ‰ìƒì€ Grab ì‹œì‘ ì‹œ ì´ë¯¸ (0, 255, 255)ë¡œ ì„¤ì •ë¨
            if self.grab_anchor_wrist:
                dx = wx - self.grab_anchor_wrist[0]
                dy = wy - self.grab_anchor_wrist[1]
                
                self.ui_x = int(self.grab_anchor_ui[0] + dx * MOVE_SCALE)
                self.ui_y = int(self.grab_anchor_ui[1] + dy * MOVE_SCALE)
                
                # í™”ë©´ ì´íƒˆ ë°©ì§€
                self.ui_x = max(50, min(self.w-50, self.ui_x))
                self.ui_y = max(50, min(self.h-50, self.ui_y))
            
            self.prev_hand_pos = (cx, cy)
            return # Grab ì¤‘ì—ëŠ” í•˜ìœ„ ë¡œì§ ì°¨ë‹¨ (ì„ íƒ/ì¤Œ/ìŠ¤ì™€ì´í”„ ê¸ˆì§€)

        # =========================================================
        # [ìš°ì„ ìˆœìœ„ 2] UI ì„ íƒ/í•´ì œ (Zì¶• í„°ì¹˜)
        # =========================================================
        # Zì¶• ê¹Šì´ ì •ë³´ë¥¼ í™œìš©í•œ ê°€ìƒ í„°ì¹˜ íŒì •
        is_touching = idx_tip.z < Z_TOUCH_THRESHOLD
        
        if current_time - self.last_action_time > 1.0: # ë””ë°”ìš´ìŠ¤ 1ì´ˆ
            if is_touching:
                if self.is_point_in_ui(ix, iy):
                    if not self.is_selected:
                        self.is_selected = True
                        self.ui_color = (0, 255, 0)
                        self.last_action_time = current_time
                        print("ğŸ‘† Selected (Z-Touch)")
                else:
                    if self.is_selected:
                        self.is_selected = False
                        self.ui_color = (255, 0, 0)
                        self.last_action_time = current_time
                        print("ğŸš« Deselected (Z-Touch)")
        
        # ì„ íƒëœ ìƒíƒœì—ì„œ ì—„ì§€ ì œìŠ¤ì²˜ (í™•ì¸/ì·¨ì†Œ)
        if self.is_selected and ai_conf > CONFIDENCE_THRESHOLD:
            if ai_label == "Thumb Up":
                cv2.putText(image, "CONFIRMED", (self.ui_x - 60, self.ui_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)
            elif ai_label == "Thumb Down":
                self.is_selected = False
                self.ui_color = (255, 0, 0)
                self.last_action_time = current_time

        # =========================================================
        # [ìš°ì„ ìˆœìœ„ 3] ì¤Œ & ìŠ¤ì™€ì´í”„ (ì„ íƒ ìƒíƒœì—ì„œë§Œ)
        # =========================================================
        if self.is_selected:
            # ì¤Œ ì œìŠ¤ì²˜ (ë‘ ì†ê°€ë½)
            if ai_conf > CONFIDENCE_THRESHOLD:
                if ai_label == "Zooming In With Two Fingers":
                    self.ui_size = min(400, self.ui_size + 8)
                elif ai_label == "Zooming Out With Two Fingers":
                    self.ui_size = max(100, self.ui_size - 8)

            # ìŠ¤ì™€ì´í”„ (ì† ì¤‘ì‹¬ì˜ ë¹ ë¥¸ ì´ë™)
            if current_time - self.last_action_time > ACTION_COOLDOWN:
                if self.prev_hand_pos is not None:
                    # ì† ì¤‘ì‹¬ (9ë²ˆ)ì˜ ì´ë™ ë²¡í„°
                    vx = cx - self.prev_hand_pos[0]
                    vy = cy - self.prev_hand_pos[1]
                    SWIPE_THRESH = 40 # ì„ê³„ê°’ ìƒí–¥ ì¡°ì • (ì˜¤ì‘ë™ ë°©ì§€)
                    
                    if abs(vx) > SWIPE_THRESH or abs(vy) > SWIPE_THRESH:
                        # ë” í° ì´ë™ ì¶•ìœ¼ë¡œ ìŠ¤ì™€ì´í”„ íŒì •
                        if abs(vx) > abs(vy):
                            self.ui_x += int(vx * 1.5)
                        else:
                            self.ui_y += int(vy * 1.5)
                            
                        # í™”ë©´ ì´íƒˆ ë°©ì§€
                        self.ui_x = max(50, min(self.w-50, self.ui_x))
                        self.ui_y = max(50, min(self.h-50, self.ui_y))
                            
                        self.last_action_time = current_time
                        print("ğŸ’¨ Swiped (Hand Center Movement)")

        # ë‹¤ìŒ í”„ë ˆì„ì„ ìœ„í•´ í˜„ì¬ ì† ì¤‘ì‹¬ ì¢Œí‘œ ì €ì¥
        self.prev_hand_pos = (cx, cy)

    def draw(self, image):
        """ UI ë°•ìŠ¤ ë° ìƒíƒœ í‘œì‹œ ê·¸ë¦¬ê¸° """
        half = self.ui_size // 2
        top_left = (self.ui_x - half, self.ui_y - half)
        bottom_right = (self.ui_x + half, self.ui_y + half)
        
        cv2.rectangle(image, top_left, bottom_right, self.ui_color, -1)
        cv2.rectangle(image, top_left, bottom_right, (255, 255, 255), 3)
        
        status = "Grabbing" if self.is_grabbing else ("Selected" if self.is_selected else "Idle")
        cv2.putText(image, status, (self.ui_x - 40, self.ui_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

# ================= ğŸ¥ ë©”ì¸ ë£¨í”„ =================
def run():
    print(f"Loading Model: {MODEL_PATH}...")
    try:
        # GPU ì‚¬ìš©ì„ ì„ í˜¸í•˜ëŠ” ê²½ìš° CUDAExecutionProviderë¥¼ ë¨¼ì € ì‹œë„
        session = ort.InferenceSession(MODEL_PATH, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
        input_name = session.get_inputs()[0].name
        print("âœ… Model Loaded!")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return

    # MediaPipe ì´ˆê¸°í™”
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    
    # ì›¹ìº  ì´ˆê¸°í™” (1280x720 í•´ìƒë„ ì„¤ì •)
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

    history_buffer = deque(maxlen=MAX_SEQ_LEN)
    manager = InteractionManager(1280, 720)
    prev_time = 0
    
    # MediaPipe Hands ëª¨ë¸ ì‹¤í–‰
    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:
        while cap.isOpened():
            success, image = cap.read()
            if not success: break
            
            # ì¢Œìš° ë°˜ì „ ë° RGB ë³€í™˜
            image = cv2.flip(image, 1)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = hands.process(image_rgb)
            
            landmarks_flat = None
            curr_label = "No gesture"
            curr_conf = 0.0
            
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                    
                    # ëœë“œë§ˆí¬ ì¶”ì¶œ ë° ë²„í¼ ì¶”ê°€
                    coords = []
                    for lm in hand_landmarks.landmark:
                        coords.extend([lm.x, lm.y, lm.z])
                    landmarks_flat = np.array(coords, dtype=np.float32)
                    history_buffer.append(landmarks_flat)
                    
                    # ìµœì†Œ ê¸¸ì´(15í”„ë ˆì„) ë„ë‹¬ ì‹œ ì¶”ë¡  ì‹œì‘
                    if len(history_buffer) >= 15:
                        input_data = preprocess_buffer(history_buffer)
                        outputs = session.run(None, {input_name: input_data})[0]
                        probs = softmax(outputs)
                        idx = np.argmax(probs)
                        curr_label = TARGET_LABELS[idx]
                        curr_conf = probs[0][idx]
                        
                        # ì¸í„°ë™ì…˜ ê´€ë¦¬ì í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
                        manager.process(curr_label, curr_conf, hand_landmarks, image)
                    break # í•œ ì†ë§Œ ì²˜ë¦¬
            else:
                # ì†ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë°ì´í„° ì¶”ê°€ (ì‹œí€€ìŠ¤ ìœ ì§€)
                if landmarks_flat is None: landmarks_flat = np.zeros(63, dtype=np.float32)
                history_buffer.append(landmarks_flat)
                manager.prev_hand_pos = None # ìŠ¤ì™€ì´í”„ ë°©ì§€

            # UI ë° ëŒ€ì‹œë³´ë“œ ê·¸ë¦¬ê¸°
            manager.draw(image)
            
            # FPS ê³„ì‚°
            curr_time = time.time()
            fps = 1 / (curr_time - prev_time) if prev_time != 0 else 0
            prev_time = curr_time
            
            draw_dashboard(image, curr_label, curr_conf, fps)

            # í™”ë©´ ì¶œë ¥ ë° ì¢…ë£Œ ì¡°ê±´
            cv2.imshow('HoloTouch Final Improved (Industrial Grade)', image)
            if cv2.waitKey(1) & 0xFF == 27: break # ESC í‚¤ë¡œ ì¢…ë£Œ
            
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    run()